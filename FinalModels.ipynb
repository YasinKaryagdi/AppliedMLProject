{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPOlHIdoILDm"
   },
   "source": [
    "# **This is where the models are build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765712268995,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "vkbmrwSfIPcp"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ud1ltphIQf-"
   },
   "source": [
    "## Model 1\n",
    "Model one is a relatively simple model following the alexnet architecture in a small version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1765712269024,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "ld9oYcprIRee"
   },
   "outputs": [],
   "source": [
    "class SIMPLE1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SIMPLE1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, stride = 1, bias=False)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 164, 3, padding=1, bias=False)\n",
    "        self.conv4_bn = nn.BatchNorm2d(164)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(164, 176, 3, padding=1, bias=False)\n",
    "        self.conv5_bn = nn.BatchNorm2d(176)\n",
    "\n",
    "        # unchanged: 176 * 8 * 8 = 11264\n",
    "        self.fc1 = nn.Linear(11264, 200, bias=False)\n",
    "        self.fc1_bn = nn.BatchNorm1d(200)\n",
    "\n",
    "    def get_logits(self, x):\n",
    "        x = (x - 0.5) * 2.0\n",
    "\n",
    "        #conv 1\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2) #256 -> 128\n",
    "\n",
    "        #conv 2\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2) #128 -> 64\n",
    "\n",
    "        #conv 3\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2) #64 -> 32\n",
    "\n",
    "        # Conv 4\n",
    "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)   # 32 -> 16\n",
    "\n",
    "        # Conv 5\n",
    "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
    "        x = F.max_pool2d(x, 2)   # 16 -> 8\n",
    "\n",
    "        # x is now (batch, 176, 8, 8)\n",
    "        x = torch.flatten(x, 1)  # (batch, 11264)\n",
    "\n",
    "        logits = self.fc1_bn(self.fc1(x))\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.get_logits(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acgsbOKrOHYj"
   },
   "source": [
    "## **Model2: more efficient FC layer, Dropout added and double convolution blocks**\n",
    "above the FC layer is using a flattening method, though it works its verry inefficient, in networks like resnet and efficientnet a global averagepooling filter is used. Another interesting feature that comes from alexnet is dropout, which prevents overfitting by dropping parameters by chance in training. One further improvement over the above model is the use of some extra conv layers in between pooling to capture more information, but scaling slower.\n",
    "**GAP:** Generalizes location (Feature location doesnt matter), perfect for classification, especially of birds where orientation/location doesnt matter. Also reduces parameters by a ton (fc layer goes from millions to thousands). Adaptive pooling is used because it doesnt break on changes before.\n",
    "**Dropout:** Prevents overfitting by forcing the model to learn form different features and not just depend on one (during training randomly drops features). Potential to use lower dropout for first few epochs then higher later.\n",
    "**Stageblocks:** Stacks double convolution layers to enrich information before sizing up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1765712269030,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "Ffru77W9PMUy"
   },
   "outputs": [],
   "source": [
    "class CLASSIC1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLASSIC1, self).__init__()\n",
    "\n",
    "        # 5 stages with double conv + pooling\n",
    "        self.stage1 = self.conv_block(3, 32)\n",
    "        self.stage2 = self.conv_block(32, 64)\n",
    "        self.stage3 = self.conv_block(64, 128)\n",
    "        self.stage4 = self.conv_block(128, 256) #adapt to 256 and 512 to conform to memory norms (powers of 2)\n",
    "        self.stage5 = self.conv_block(256, 512)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)   # Global Average Pooling reduces parameters and betters generalization\n",
    "        self.dropout = nn.Dropout(p=0.4) # Prevents overfitting, with some reduced probability to allow quicker learning and not over-regularize\n",
    "        self.fc1 = nn.Linear(512, 200)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_ch, out_ch): #first building block for conv layers (stack of 2)\n",
    "        # Standard pattern: Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> MaxPool\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): #replaced forward + logit with just forward\n",
    "        x = (x - 0.5) * 2.0\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        # x is now (Batch_Size, 512, 1, 1) assuming input was 32x32 (5 max pools)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc1(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OibKS91wX1Ok"
   },
   "source": [
    "## MODEL3: RESIDUALS! and some efficientnet magic\n",
    "This is where the big stuff starts, we start adding residual connections, basically skips that make networks better to train and able to go deeper.\n",
    "To make this a little easier to work with, instead of defining blocks inside of the class we use extra classes for our different types. Now it becomes clear that every neural network is just a bunch of building blocks that can be stacked on top of eachother, like the first residual block is a mini network by itself.\n",
    "^CLASSICRES\n",
    "\n",
    "Now we're gonna go a bit deeper on the developments that based themselves on resnet, changes that will be made are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1765712269061,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "lLuQSZdZXzzY"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        # shortcut: identity if channels match, otherwise 1x1 conv\n",
    "        self.shortcut = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class CLASSICRES(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stage-level residual blocks\n",
    "        self.stage1 = ResidualBlock(3, 32)       # 256 -> 128\n",
    "        self.stage2 = ResidualBlock(32, 64)      # 128 -> 64\n",
    "        self.stage3 = ResidualBlock(64, 128)     # 64 -> 32\n",
    "        self.stage4 = ResidualBlock(128, 256)    # 32 -> 16\n",
    "        self.stage5 = ResidualBlock(256, 512)    # 16 -> 8\n",
    "\n",
    "        # Classifier\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "\n",
    "        x = self.gap(x)                  # (B, 512, 1, 1)\n",
    "        x = torch.flatten(x, 1)          # (B, 512)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jqiXCz0zkEn"
   },
   "source": [
    "### Modernizing resnet\n",
    "We add squeeze and excitations modules, inspired by implementations of efficientnet and SENet. These blocks are used as a sort of attention mechanism where it uses global average to get an idea of the importance of a channel and then expands these channels and weight the original feature maps. This is a cheap improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1765712269079,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "Zp_dVCFZ1dwl"
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.activation = nn.SiLU()       # <-- changed from ReLU\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # Squeeze: global average pooling\n",
    "        y = x.mean(dim=(2, 3))           # (B, C)\n",
    "        # Excitation: MLP\n",
    "        y = self.fc2(self.activation(self.fc1(y)))  # (B, C)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        # Scale: multiply original feature map\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765712269082,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "nA4NcHFp3CI3"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_se=False, use_pool=False):\n",
    "        super().__init__()\n",
    "        self.use_se = use_se\n",
    "        self.use_pool = use_pool\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.shortcut = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.act = nn.SiLU(inplace=True)  # <-- changed from ReLU\n",
    "        if use_pool:\n",
    "            self.pool = nn.MaxPool2d(2)\n",
    "        if use_se:\n",
    "            self.se = SEBlock(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "\n",
    "        if self.use_se:\n",
    "            out = self.se(out)\n",
    "\n",
    "        out = self.act(out)\n",
    "        if self.use_pool:\n",
    "            out = self.pool(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1765712269108,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "7Nyk0sLPnuG2"
   },
   "outputs": [],
   "source": [
    "class MODERNRES(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stage-level residual blocks\n",
    "        self.stage1 = ResidualBlock(3, 32, use_se=True)\n",
    "        self.stage2 = ResidualBlock(32, 64, use_se=True)\n",
    "        self.stage3 = ResidualBlock(64, 96, use_se=True)\n",
    "        self.stage4 = ResidualBlock(96, 128, use_se=True)\n",
    "        self.stage5 = ResidualBlock(128, 160, use_se=True)\n",
    "\n",
    "        # Classifier\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(160, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1765712269128,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "_nsE3rsrmRq1"
   },
   "outputs": [],
   "source": [
    "class MODERNRESDEEP(nn.Module):\n",
    "    def __init__(self, num_classes=200):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stage-level residual blocks\n",
    "        self.stage1 = ResidualBlock(3, 32, use_se=True)\n",
    "        self.stage2 = ResidualBlock(32, 64, use_se=True, use_pool = False)\n",
    "        self.stage3 = ResidualBlock(64, 96, use_se=True)\n",
    "        self.stage4 = ResidualBlock(96, 128, use_se=True, use_pool = False)\n",
    "        self.stage5 = ResidualBlock(128, 160, use_se=True)\n",
    "        self.stage6 = ResidualBlock(160, 192, use_se=True, use_pool = False)\n",
    "        self.stage7 = ResidualBlock(192, 224, use_se=True)\n",
    "        self.stage8 = ResidualBlock(224, 256, use_se=True, use_pool = False)\n",
    "        self.stage9 = ResidualBlock(256, 288, use_se=True)\n",
    "\n",
    "        # Classifier\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(288, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n",
    "\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31823,
     "status": "ok",
     "timestamp": 1765712300956,
     "user": {
      "displayName": "Mark Schuddeboom",
      "userId": "03719376082377648878"
     },
     "user_tz": -60
    },
    "id": "-FiepnNul9s9",
    "outputId": "3b22aa48-8ce2-46ad-83c7-ed522994c04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 256]              96\n",
      "            Conv2d-2         [-1, 32, 256, 256]             864\n",
      "       BatchNorm2d-3         [-1, 32, 256, 256]              64\n",
      "              SiLU-4         [-1, 32, 256, 256]               0\n",
      "            Conv2d-5         [-1, 32, 256, 256]           9,216\n",
      "       BatchNorm2d-6         [-1, 32, 256, 256]              64\n",
      "            Linear-7                    [-1, 2]              66\n",
      "              SiLU-8                    [-1, 2]               0\n",
      "            Linear-9                   [-1, 32]              96\n",
      "          Sigmoid-10                   [-1, 32]               0\n",
      "          SEBlock-11         [-1, 32, 256, 256]               0\n",
      "             SiLU-12         [-1, 32, 256, 256]               0\n",
      "    ResidualBlock-13         [-1, 32, 256, 256]               0\n",
      "           Conv2d-14         [-1, 64, 256, 256]           2,048\n",
      "           Conv2d-15         [-1, 64, 256, 256]          18,432\n",
      "      BatchNorm2d-16         [-1, 64, 256, 256]             128\n",
      "             SiLU-17         [-1, 64, 256, 256]               0\n",
      "           Conv2d-18         [-1, 64, 256, 256]          36,864\n",
      "      BatchNorm2d-19         [-1, 64, 256, 256]             128\n",
      "           Linear-20                    [-1, 4]             260\n",
      "             SiLU-21                    [-1, 4]               0\n",
      "           Linear-22                   [-1, 64]             320\n",
      "          Sigmoid-23                   [-1, 64]               0\n",
      "          SEBlock-24         [-1, 64, 256, 256]               0\n",
      "             SiLU-25         [-1, 64, 256, 256]               0\n",
      "    ResidualBlock-26         [-1, 64, 256, 256]               0\n",
      "           Conv2d-27         [-1, 96, 256, 256]           6,144\n",
      "           Conv2d-28         [-1, 96, 256, 256]          55,296\n",
      "      BatchNorm2d-29         [-1, 96, 256, 256]             192\n",
      "             SiLU-30         [-1, 96, 256, 256]               0\n",
      "           Conv2d-31         [-1, 96, 256, 256]          82,944\n",
      "      BatchNorm2d-32         [-1, 96, 256, 256]             192\n",
      "           Linear-33                    [-1, 6]             582\n",
      "             SiLU-34                    [-1, 6]               0\n",
      "           Linear-35                   [-1, 96]             672\n",
      "          Sigmoid-36                   [-1, 96]               0\n",
      "          SEBlock-37         [-1, 96, 256, 256]               0\n",
      "             SiLU-38         [-1, 96, 256, 256]               0\n",
      "    ResidualBlock-39         [-1, 96, 256, 256]               0\n",
      "           Conv2d-40        [-1, 128, 256, 256]          12,288\n",
      "           Conv2d-41        [-1, 128, 256, 256]         110,592\n",
      "      BatchNorm2d-42        [-1, 128, 256, 256]             256\n",
      "             SiLU-43        [-1, 128, 256, 256]               0\n",
      "           Conv2d-44        [-1, 128, 256, 256]         147,456\n",
      "      BatchNorm2d-45        [-1, 128, 256, 256]             256\n",
      "           Linear-46                    [-1, 8]           1,032\n",
      "             SiLU-47                    [-1, 8]               0\n",
      "           Linear-48                  [-1, 128]           1,152\n",
      "          Sigmoid-49                  [-1, 128]               0\n",
      "          SEBlock-50        [-1, 128, 256, 256]               0\n",
      "             SiLU-51        [-1, 128, 256, 256]               0\n",
      "    ResidualBlock-52        [-1, 128, 256, 256]               0\n",
      "           Conv2d-53        [-1, 160, 256, 256]          20,480\n",
      "           Conv2d-54        [-1, 160, 256, 256]         184,320\n",
      "      BatchNorm2d-55        [-1, 160, 256, 256]             320\n",
      "             SiLU-56        [-1, 160, 256, 256]               0\n",
      "           Conv2d-57        [-1, 160, 256, 256]         230,400\n",
      "      BatchNorm2d-58        [-1, 160, 256, 256]             320\n",
      "           Linear-59                   [-1, 10]           1,610\n",
      "             SiLU-60                   [-1, 10]               0\n",
      "           Linear-61                  [-1, 160]           1,760\n",
      "          Sigmoid-62                  [-1, 160]               0\n",
      "          SEBlock-63        [-1, 160, 256, 256]               0\n",
      "             SiLU-64        [-1, 160, 256, 256]               0\n",
      "    ResidualBlock-65        [-1, 160, 256, 256]               0\n",
      "           Conv2d-66        [-1, 192, 256, 256]          30,720\n",
      "           Conv2d-67        [-1, 192, 256, 256]         276,480\n",
      "      BatchNorm2d-68        [-1, 192, 256, 256]             384\n",
      "             SiLU-69        [-1, 192, 256, 256]               0\n",
      "           Conv2d-70        [-1, 192, 256, 256]         331,776\n",
      "      BatchNorm2d-71        [-1, 192, 256, 256]             384\n",
      "           Linear-72                   [-1, 12]           2,316\n",
      "             SiLU-73                   [-1, 12]               0\n",
      "           Linear-74                  [-1, 192]           2,496\n",
      "          Sigmoid-75                  [-1, 192]               0\n",
      "          SEBlock-76        [-1, 192, 256, 256]               0\n",
      "             SiLU-77        [-1, 192, 256, 256]               0\n",
      "    ResidualBlock-78        [-1, 192, 256, 256]               0\n",
      "           Conv2d-79        [-1, 224, 256, 256]          43,008\n",
      "           Conv2d-80        [-1, 224, 256, 256]         387,072\n",
      "      BatchNorm2d-81        [-1, 224, 256, 256]             448\n",
      "             SiLU-82        [-1, 224, 256, 256]               0\n",
      "           Conv2d-83        [-1, 224, 256, 256]         451,584\n",
      "      BatchNorm2d-84        [-1, 224, 256, 256]             448\n",
      "           Linear-85                   [-1, 14]           3,150\n",
      "             SiLU-86                   [-1, 14]               0\n",
      "           Linear-87                  [-1, 224]           3,360\n",
      "          Sigmoid-88                  [-1, 224]               0\n",
      "          SEBlock-89        [-1, 224, 256, 256]               0\n",
      "             SiLU-90        [-1, 224, 256, 256]               0\n",
      "    ResidualBlock-91        [-1, 224, 256, 256]               0\n",
      "           Conv2d-92        [-1, 256, 256, 256]          57,344\n",
      "           Conv2d-93        [-1, 256, 256, 256]         516,096\n",
      "      BatchNorm2d-94        [-1, 256, 256, 256]             512\n",
      "             SiLU-95        [-1, 256, 256, 256]               0\n",
      "           Conv2d-96        [-1, 256, 256, 256]         589,824\n",
      "      BatchNorm2d-97        [-1, 256, 256, 256]             512\n",
      "           Linear-98                   [-1, 16]           4,112\n",
      "             SiLU-99                   [-1, 16]               0\n",
      "          Linear-100                  [-1, 256]           4,352\n",
      "         Sigmoid-101                  [-1, 256]               0\n",
      "         SEBlock-102        [-1, 256, 256, 256]               0\n",
      "            SiLU-103        [-1, 256, 256, 256]               0\n",
      "   ResidualBlock-104        [-1, 256, 256, 256]               0\n",
      "          Conv2d-105        [-1, 288, 256, 256]          73,728\n",
      "          Conv2d-106        [-1, 288, 256, 256]         663,552\n",
      "     BatchNorm2d-107        [-1, 288, 256, 256]             576\n",
      "            SiLU-108        [-1, 288, 256, 256]               0\n",
      "          Conv2d-109        [-1, 288, 256, 256]         746,496\n",
      "     BatchNorm2d-110        [-1, 288, 256, 256]             576\n",
      "          Linear-111                   [-1, 18]           5,202\n",
      "            SiLU-112                   [-1, 18]               0\n",
      "          Linear-113                  [-1, 288]           5,472\n",
      "         Sigmoid-114                  [-1, 288]               0\n",
      "         SEBlock-115        [-1, 288, 256, 256]               0\n",
      "            SiLU-116        [-1, 288, 256, 256]               0\n",
      "   ResidualBlock-117        [-1, 288, 256, 256]               0\n",
      "AdaptiveAvgPool2d-118            [-1, 288, 1, 1]               0\n",
      "         Dropout-119                  [-1, 288]               0\n",
      "          Linear-120                  [-1, 200]          57,800\n",
      "================================================================\n",
      "Total params: 5,186,690\n",
      "Trainable params: 5,186,690\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 6480.03\n",
      "Params size (MB): 19.79\n",
      "Estimated Total Size (MB): 6500.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary  # pip install torchsummary\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize and move model to device\n",
    "print_model = MODERNRESDEEP()\n",
    "\n",
    "# Print model summary\n",
    "# Input shape: 3 channels, 256x256 image\n",
    "summary(print_model, (3, 256, 256))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPy5WG09bPZa5MUNO8YgowV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
