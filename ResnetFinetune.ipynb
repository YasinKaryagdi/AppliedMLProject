{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/YasinKaryagdi/AppliedMLProject/blob/ResnetColab/ResnetFinetune.ipynb",
      "authorship_tag": "ABX9TyOWlPt4++41uO30A1zekB97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasinKaryagdi/AppliedMLProject/blob/ResnetColab/ResnetFinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jKm4ZAjDpgth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b241e84-2f75-4d1c-b6cd-39d0d84a4c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AppliedMLProject' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://YasinKaryagdi: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@github.com/YasinKaryagdi/AppliedMLProject.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually add augmented set+augmented csv for now + test set csv"
      ],
      "metadata": {
        "id": "fyE0D1e-NR5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from imutils import paths\n",
        "from pathlib import Path\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Jpb3XBcdyvwr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cwd = Path.cwd()\n",
        "gitpath = cwd / \"AppliedMLProject\"\n",
        "dirpath = gitpath / \"aml-2025-feathers-in-focus\"\n",
        "train_images_csv = dirpath / \"train_images.csv\"\n",
        "train_images_folder = dirpath / \"train_images\"\n",
        "image_classes = dirpath / \"class_names.npy\"\n",
        "drive_path = cwd / \"drive\" / \"MyDrive\" / \"Machinelearning files\"\n",
        "val_images_csv = cwd / \"validate_split.csv\"\n"
      ],
      "metadata": {
        "id": "NaI9baZX_ab5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining model and training variables\n",
        "#use augmented trainingset\n",
        "use_augmented = True\n",
        "#model\n",
        "model_name = 'resnet'\n",
        "#training batchsize\n",
        "train_batch_size = 32\n",
        "#validation & testing batchsize\n",
        "val_batch_size = 64\n",
        "#Epochs\n",
        "num_epochs = 25\n",
        "#feature extraction option\n",
        "feature_extract = False\n",
        "#resize to:\n",
        "size = (256,256)\n",
        "#use pretrained or not\n",
        "use_pretrained = True\n",
        "classes = np.load(image_classes, allow_pickle=True).item()\n",
        "num_classes = len(classes)\n",
        "#train-test split\n",
        "split = 0.85"
      ],
      "metadata": {
        "id": "_pU3X7eQ338C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, base_dir, transform=None, return_id=False):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.base_dir = base_dir\n",
        "        self.transform = transform\n",
        "        self.return_id = return_id  # Useful for test set where no labels exist\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # extract fields\n",
        "        img_id = row['id'] if self.return_id else None\n",
        "        relative_path = row['image_path'].lstrip('/')  # safe\n",
        "        label = row['label'] - 1   # shift to 0-based indexing\n",
        "\n",
        "        # build full path\n",
        "        img_path = os.path.join(self.base_dir, relative_path)\n",
        "\n",
        "        # load\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # optionally return id\n",
        "        if self.return_id:\n",
        "            return image, label, img_id\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "L3-GYuBXHV6Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "UNhVRp2SZVX5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                schedular=None,\n",
        "                num_epochs=10,\n",
        "                device=\"cuda\"):\n",
        "    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "    since = time.time()\n",
        "    model.to(device)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    val_acc_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "        for phase in ['train', 'val']:\n",
        "              if phase == 'train':\n",
        "                  model.train()  # Set model to training mode\n",
        "              else:\n",
        "                  model.eval()   # Set model to evaluate mode\n",
        "\n",
        "              running_loss = 0.0\n",
        "              running_corrects = 0\n",
        "\n",
        "              # Iterate over data.\n",
        "              for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "\n",
        "                  # zero the parameter gradients\n",
        "                  optimizer.zero_grad()\n",
        "\n",
        "                  # forward\n",
        "                  # track history if only in train\n",
        "                  with torch.set_grad_enabled(phase == 'train'):\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                      # backward + optimize only if in training phase\n",
        "                      if phase == 'train':\n",
        "                          loss.backward()\n",
        "                          optimizer.step()\n",
        "\n",
        "                  # statistics\n",
        "                  running_loss += loss.item() * inputs.size(0)\n",
        "                  running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "              epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n",
        "              epoch_acc = running_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "              print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "              # deep copy the model\n",
        "              if phase == 'val' and epoch_acc > best_acc:\n",
        "                  best_acc = epoch_acc\n",
        "                  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              if phase == 'val':\n",
        "                  val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history\n"
      ],
      "metadata": {
        "id": "YseV73jKZVN-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define some standard transformations\n",
        "transformations = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((size)),\n",
        "    transforms.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5))\n",
        "    ])\n",
        "transformations_resnet = models.ResNet152_Weights.IMAGENET1K_V1.transforms()\n",
        "## Probably better to follow the original resnet transformations\n",
        "#See: (model.ResNet152_Weights.IMAGENET1K_V1.transforms)"
      ],
      "metadata": {
        "id": "8f7j0yqN9Upj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_augmented == False:\n",
        "  full_dataset = CSVDataset(\n",
        "      csv_file=str(dirpath / \"train_images.csv\"),\n",
        "      base_dir=str(dirpath),\n",
        "      transform = transformations_resnet,\n",
        "      return_id=False\n",
        "  )\n",
        "  loader = DataLoader(full_dataset, batch_size=train_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "l-tCV3MxO05B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_augmented == True:\n",
        "  train_dataset = CSVDataset(\n",
        "      csv_file=str(cwd / \"train_augmented.csv\"),\n",
        "      base_dir=str(drive_path),\n",
        "      transform = transformations_resnet,\n",
        "      return_id=False\n",
        "  )\n",
        "  val_dataset = CSVDataset(\n",
        "      csv_file=str(val_images_csv),\n",
        "      base_dir=str(dirpath),\n",
        "      transform = transformations_resnet,\n",
        "      return_id=False\n",
        "  )"
      ],
      "metadata": {
        "id": "nPhB2qnJRU8X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "if model_name == \"resnet\":\n",
        "  \"\"\"Resnet152\"\"\"\n",
        "  ResNet_Weights = models.ResNet152_Weights.DEFAULT\n",
        "  transforms_resnet = ResNet_Weights.transforms()\n",
        "  if use_pretrained:\n",
        "    model_ft = models.resnet152(weights=ResNet_Weights.IMAGENET1K_V1)\n",
        "  else:\n",
        "    model_ft = models.resnet152()\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "  model_ft.fc = nn.Linear(num_ftrs, num_classes)"
      ],
      "metadata": {
        "id": "FqSjkmbPIWfS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-validation split\n",
        "# Split into train (85%) and validation (15%)\n",
        "if use_augmented == False:\n",
        "  train_size = int(split * len(full_dataset))\n",
        "  val_size = len(full_dataset) - train_size\n",
        "  train_dataset, val_dataset = random_split(\n",
        "      full_dataset,\n",
        "      [train_size, val_size],\n",
        "      generator=torch.Generator().manual_seed(8)\n",
        "  )"
      ],
      "metadata": {
        "id": "i4fYskKDSYsg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "2vCiR-hbVSTy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if it does what I want it to\n",
        "# 1. Check dataset length\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n",
        "\n",
        "# 2. Get a single sample\n",
        "image, label = train_dataset[0]\n",
        "print(f\"Single image shape: {image.shape}\")  # Should be [3, 224, 224]\n",
        "print(f\"Single image type: {type(image)}\")   # Should be torch.Tensor\n",
        "print(f\"Single label: {label}\")              # Should be an integer\n",
        "print(f\"Label type: {type(label)}\")          # Should be int or numpy.int64\n",
        "\n",
        "# 3. Check a batch from the DataLoader\n",
        "batch_images, batch_labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch images shape: {batch_images.shape}\")  # Should be [32, 3, 224, 224]\n",
        "print(f\"Batch images type: {type(batch_images)}\")     # Should be torch.Tensor\n",
        "print(f\"Batch images dtype: {batch_images.dtype}\")    # Should be torch.float32\n",
        "print(f\"Batch labels shape: {batch_labels.shape}\")    # Should be [32]\n",
        "print(f\"Batch labels type: {type(batch_labels)}\")     # Should be torch.Tensor\n",
        "print(f\"Batch labels dtype: {batch_labels.dtype}\")    # Could be torch.int64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwTdcbP7H8Xu",
        "outputId": "4b6db7da-8d9d-4687-a728-f669f5b1481d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 28260\n",
            "Single image shape: torch.Size([3, 224, 224])\n",
            "Single image type: <class 'torch.Tensor'>\n",
            "Single label: 41\n",
            "Label type: <class 'numpy.int64'>\n",
            "\n",
            "Batch images shape: torch.Size([32, 3, 224, 224])\n",
            "Batch images type: <class 'torch.Tensor'>\n",
            "Batch images dtype: torch.float32\n",
            "Batch labels shape: torch.Size([32])\n",
            "Batch labels type: <class 'torch.Tensor'>\n",
            "Batch labels dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DartSzDJXDPX",
        "outputId": "13a2cb18-971e-4105-93be-c1a67afbda04"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#put model on device\n",
        "model_ft = model_ft.to(device)"
      ],
      "metadata": {
        "id": "w-W26P52XRB_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gather optimizable parameters\n",
        "params_to_update = model_ft.parameters()\n",
        "#Design optimzer\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "# Setup the loss func\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1pT9gfszYnX8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate\n",
        "model_trained, hist = train_model(model_ft,\n",
        "                            train_loader,\n",
        "                            val_loader,\n",
        "                            criterion,\n",
        "                            optimizer,\n",
        "                            schedular=None,\n",
        "                            num_epochs=num_epochs,\n",
        "                            device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "Prs-oDveZHBb",
        "outputId": "3ef6d2b4-7194-4544-9c3e-8c5e37b01136"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/884 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3554454333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_trained, hist = train_model(model_ft, \n\u001b[0m\u001b[1;32m      3\u001b[0m                             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-601278071.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, schedular, num_epochs, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m               \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4190594918.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_trained.state_dict(), \"best_model.pth\")"
      ],
      "metadata": {
        "id": "sU_IyDNmtbKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    ids_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, img_ids in tqdm(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            #Readd 1 to label to give right predictions\n",
        "            preds_list.extend(preds.cpu().numpy()+1)\n",
        "            ids_list.extend(img_ids.numpy())\n",
        "\n",
        "    return ids_list, preds_list"
      ],
      "metadata": {
        "id": "zusGMHLAuFWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Test set\n",
        "test_dataset = CSVDataset(\n",
        "    csv_file=str(dirpath / \"test_images_path.csv\"),\n",
        "    base_dir=str(dirpath),\n",
        "    # transform = transforms_resnet,\n",
        "    transform = transformations_resnet,\n",
        "    return_id=True\n",
        ")\n",
        "test_image_ids = test_dataset.df['id'].tolist()\n",
        "#Create dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)\n",
        "\n",
        "# # 3. Check a batch from the DataLoader\n",
        "# batch_images, batch_labels = next(iter(test_loader))\n",
        "# print(f\"\\nBatch images shape: {batch_images.shape}\")  # Should be [32, 3, 224, 224]\n",
        "# print(f\"Batch images type: {type(batch_images)}\")     # Should be torch.Tensor\n",
        "# print(f\"Batch images dtype: {batch_images.dtype}\")    # Should be torch.float32\n",
        "# print(f\"Batch labels shape: {batch_labels.shape}\")    # Should be [32]\n",
        "# print(f\"Batch labels type: {type(batch_labels)}\")     # Should be torch.Tensor\n",
        "# print(f\"Batch labels dtype: {batch_labels.dtype}\")    # Could be torch.int64"
      ],
      "metadata": {
        "id": "uO8WdS_LtjId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "finetuned_model = models.resnet152()\n",
        "num_ftrs = finetuned_model.fc.in_features\n",
        "finetuned_model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "finetuned_model.load_state_dict(torch.load(\"/content/best_model.pth\"))\n",
        "finetuned_model.to(device)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N6sGXPTl6rRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run model\n",
        "test_ids, test_preds = predict(finetuned_model, test_loader, device=device)"
      ],
      "metadata": {
        "id": "HkkdiOTiw258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate submission.csv\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"label\": test_preds\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission2.csv\", index=False)"
      ],
      "metadata": {
        "id": "gKXTisekx_PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWHMPHAhzRcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}