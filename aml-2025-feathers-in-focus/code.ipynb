{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "from PIL import Image    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328637df",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This is where we preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdf34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49f002ee",
   "metadata": {},
   "source": [
    "### Load in data from directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8ceb4",
   "metadata": {},
   "source": [
    "#### Trying to do zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max height and width of dataset\n",
    "paths = pd.read_csv(\"train_images.csv\")\n",
    "\n",
    "max_w = 0\n",
    "max_h = 0\n",
    "for i in paths.index:\n",
    "    # The dot makes it so that it's relative to your current dir,\n",
    "    curr_path = paths[\"image_path\"][i]\n",
    "    rgb_img = Image.open(\".\" + curr_path).convert('RGB')\n",
    "\n",
    "    h = np.shape(rgb_img)[0]\n",
    "    w = np.shape(rgb_img)[1]\n",
    "\n",
    "    if (h > max_h):\n",
    "        max_h = h\n",
    "    if (w > max_w):\n",
    "        max_w = w\n",
    "\n",
    "max_h = max_h\n",
    "max_w = max_w\n",
    "print(\"max H: \", max_h)    \n",
    "print(\"max W: \", max_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff21b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and storing train to a vector\n",
    "# can't store it in numpy array due to diff in dimensionality\n",
    "paths = pd.read_csv(\"train_images.csv\")\n",
    "\n",
    "X = np.zeros((3926, 500, 500, 3), dtype=np.uint8)\n",
    "\n",
    "for i in paths.index:\n",
    "    # The dot makes it so that it's relative to your current dir,\n",
    "    curr_path = paths[\"image_path\"][i]\n",
    "    rgb_img = Image.open(\".\" + curr_path).convert('RGB')\n",
    "    rgb_img = np.array(rgb_img)\n",
    "    \n",
    "    # calculate the amount of padding\n",
    "    h = np.shape(rgb_img)[0]\n",
    "    w = np.shape(rgb_img)[1]\n",
    "\n",
    "    # print(\"h\", h)\n",
    "    # print(\"w\", w)\n",
    "\n",
    "    pad_left = (max_w - w) // 2\n",
    "    pad_right = max_w - w - pad_left\n",
    "\n",
    "    pad_up = (max_h - h) // 2\n",
    "    pad_down = max_h - h - pad_up\n",
    "\n",
    "    # print(\"left\", pad_left)\n",
    "    # print(\"right\", pad_right)\n",
    "    # print(\"up\", pad_up)\n",
    "    # print(\"down\", pad_down)\n",
    "\n",
    "    # Actually perform the padding\n",
    "    rgb_img = np.pad(rgb_img, pad_width=((pad_up, pad_down), (pad_left, pad_right), (0,0)))\n",
    "\n",
    "    # # calculate the amount of padding\n",
    "    # h = np.shape(rgb_img)[0]\n",
    "    # print(\"h\", h)\n",
    "    # w = np.shape(rgb_img)[1]\n",
    "    # print(\"w\", w)\n",
    "\n",
    "    # pad_left = (max_w - w) // 2\n",
    "    # pad_right = max_w - w - pad_left\n",
    "\n",
    "    # pad_up = (max_h - h) // 2\n",
    "    # pad_down = max_h - h - pad_up\n",
    "\n",
    "    # print(\"left\", pad_left)\n",
    "    # print(\"right\", pad_right)\n",
    "    # print(\"up\", pad_up)\n",
    "    # print(\"down\", pad_down)\n",
    "    # plt.imshow(rgb_img)\n",
    "\n",
    "    X[i] = rgb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)\n",
    "plt.imshow(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29062d7a",
   "metadata": {},
   "source": [
    "### Resize version of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b633d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and storing train to a vector\n",
    "# can't store it in numpy array due to diff in dimensionality\n",
    "paths = pd.read_csv(\"train_images.csv\")\n",
    "\n",
    "X = []\n",
    "for i in paths.index:\n",
    "    # The dot makes it so that it's relative to your current dir,\n",
    "    curr_path = paths[\"image_path\"][i]\n",
    "    rgb_img = Image.open(\".\" + curr_path).convert('RGB')\n",
    "\n",
    "    # Resize to 256, 256\n",
    "    width = np.shape(rgb_img)[0]\n",
    "    height = np.shape(rgb_img)[1]\n",
    "\n",
    "    if width == height:\n",
    "        rgb_img = rgb_img.resize((256,256))\n",
    "    else:\n",
    "        if width > height:\n",
    "            left = width/2 - height/2\n",
    "            right = width/2 + height/2\n",
    "            top = 0\n",
    "            bottom = height\n",
    "            rgb_img = rgb_img.crop((left,top,right,bottom))\n",
    "            rgb_img = rgb_img.resize((256,256))\n",
    "        else:\n",
    "            left = 0\n",
    "            right = width\n",
    "            top = 0\n",
    "            bottom = width\n",
    "            rgb_img = rgb_img.crop((left,top,right,bottom))\n",
    "            rgb_img = rgb_img.resize((256,256))\n",
    "    \n",
    "    X.append(rgb_img)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y = paths[\"label\"]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out if this matches nicely\n",
    "index = 35\n",
    "plt.imshow(X[index])\n",
    "print(Y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fab5a8",
   "metadata": {},
   "source": [
    "### Some exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp \n",
    "\n",
    "data = pd.read_csv(\"/Users/micolcandoni/Documents/GitHub/AppliedMLProject/aml-2025-feathers-in-focus/train_images.csv\")\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "#count how many images per label \n",
    "\n",
    "counts_labels = data['label'].value_counts().sort_index().reset_index()\n",
    "counts_labels.columns = ['Label', 'Count']\n",
    "\n",
    "\n",
    "for i in counts_labels.iterrows():\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12609c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6457bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts_labels['Label'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viualise counts in a plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15, 4))\n",
    "plt.bar(counts_labels['Label'], counts_labels['Count'], color = 'violet')\n",
    "plt.title(\"Number of Images per Bird Class\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd738bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().head()      # Most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().tail()      # Least common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21211a5",
   "metadata": {},
   "source": [
    "### Make sure all data is in the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf529e34",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe10f3",
   "metadata": {},
   "source": [
    "> Creating a validation set (from train_images.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validate_data = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    stratify= data['label'],\n",
    "    #to ensure that each bird class is represented fairly in both splits\n",
    "    random_state= 42\n",
    ")\n",
    "\n",
    "train_data.to_csv(\"train_split.csv\", index=False)\n",
    "validate_data.to_csv(\"validate_split.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f855eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validate_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.size)\n",
    "print(validate_data.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f033bf",
   "metadata": {},
   "source": [
    "> Printed to make sure that the dataset was correctly split (and to see what it looks like).\n",
    "\n",
    "- Full dataset size: 7852\n",
    "- Train set size: 6280\n",
    "- Validate set size: 1572\n",
    "\n",
    "> Now I will compute some Basic Image Data Augmentations on my train set, specifically Image Manipulation (ex:rotation, translation, shearing, flipping, cropping, noise injection, and colour space).\n",
    "\n",
    "> According to the paper, things to look out for are: overly aggressive transformations (as this can risk confusing the model), mixing across classes to create ambiguous images, and overfitting (by relying too much on augmentations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1829aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"train_split.csv\")\n",
    "\n",
    "augmentation_output = \"augmented_set\"\n",
    "os.makedirs(augmentation_output, exist_ok=True)\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "\n",
    "\n",
    "def rotate(img, degrees = 15):\n",
    "    return img.rotate(degrees)\n",
    "#rotate the image by 15 degrees\n",
    "\n",
    "\n",
    "def flip(img):\n",
    "    return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#flip image horizontally\n",
    "\n",
    "\n",
    "def brighten(img, factor = 1.3):\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    return enhancer.enhance(factor)\n",
    "#increase brightness\n",
    "\n",
    "\n",
    "def addnoise(img, amount = 10):\n",
    "    array = np.array(img).astype(np.int16)\n",
    "    noise = np.random.randint(-amount, amount, array.shape)\n",
    "    array = np.clip(array + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(array)\n",
    "#add random noise (small)\n",
    "\n",
    "\n",
    "def translate(img, shiftx = 15, shifty = 10):\n",
    "    return img.transform(\n",
    "        img.size, \n",
    "        Image.AFFINE,\n",
    "        (1, 0, shiftx, 0, 1, shifty),\n",
    "        resample = Image.BICUBIC,\n",
    "        fillcolor = (0, 0, 0)\n",
    "    )\n",
    "#do translations (shift the image slightly as shown in the paper)\n",
    "\n",
    "\n",
    "#create dictionary:\n",
    "augmentations = {\n",
    "    \"rotate\": lambda img: rotate(img, 15),\n",
    "    \"flip\": lambda img: flip(img),\n",
    "    \"brighten\": lambda img: brighten(img, 1.3),\n",
    "    \"noise\": lambda img: addnoise(img),\n",
    "    \"translate\": lambda img: translate(img, 15, 10)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#applying the 5 augmentation techniques to create new images (increase dataset size)\n",
    "\n",
    "for index, row, in train_set.iterrows():\n",
    "    image_path = row[\"image_path\"].lstrip(\"/\")\n",
    "    label = row[\"label\"]\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    base_name = os.path.basename(image_path)\n",
    "    name, extension = os.path.splitext(base_name)\n",
    "\n",
    "\n",
    "    for augmented_name, augmented_function in augmentations.items():\n",
    "        augmented_image = augmented_function(image)\n",
    "        newfile = f\"{name}_{augmented_name}{extension}\"\n",
    "        newpath = f\"{augmentation_output}/{newfile}\"\n",
    "\n",
    "        augmented_image.save(newpath)\n",
    "\n",
    "        new_rows.append({\n",
    "            \"image_path\": newpath,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "#combining the original images + new rows to create bigger dataset\n",
    "\n",
    "augmenteddf = pd.DataFrame(new_rows)\n",
    "newdf = pd.concat([train_set, augmenteddf], ignore_index=True)\n",
    "\n",
    "newdf.to_csv(\"train_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdf.head())\n",
    "print(newdf.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5116cfa",
   "metadata": {},
   "source": [
    "> printed to see what my new df looks like. Size is 37680, which confirms that augmentations worked correctly. \n",
    "\n",
    ">Initial amount = 6280 x 5 = 31400 + 6280 (original) = 37680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58353127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_augmented.csv\")\n",
    "\n",
    "# Remove leading slash ONLY \n",
    "df[\"image_path\"] = df[\"image_path\"].str.lstrip(\"/\")\n",
    "\n",
    "df.to_csv(\"train_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c053a",
   "metadata": {},
   "source": [
    "> Now I can randomly select a picture and generate the augmentations to see how the augmentations look like compared to the original. Every time I run, a new image will be selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e523a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df = pd.read_csv(\"train_augmented.csv\")\n",
    "\n",
    "#Pick a random ORIGINAL image from train_set only (not from augmented_set)\n",
    "df_original_only = df[df[\"image_path\"].str.contains(\"train_images\")]\n",
    "random_row = df_original_only.sample(1).iloc[0]\n",
    "\n",
    "orig_path = random_row[\"image_path\"]\n",
    "label = random_row[\"label\"]\n",
    "\n",
    "#Extract base name (ex: \"1250\")\n",
    "base_name = os.path.splitext(os.path.basename(orig_path))[0]\n",
    "\n",
    "#Build expected augmented filenames\n",
    "aug_folder = \"augmented_set\"\n",
    "aug_versions = {\n",
    "    \"rotate\": f\"{aug_folder}/{base_name}_rotate.jpg\",\n",
    "    \"flip\": f\"{aug_folder}/{base_name}_flip.jpg\",\n",
    "    \"brighten\": f\"{aug_folder}/{base_name}_brighten.jpg\",\n",
    "    \"noise\": f\"{aug_folder}/{base_name}_noise.jpg\",\n",
    "    \"translate\": f\"{aug_folder}/{base_name}_translate.jpg\"\n",
    "}\n",
    "\n",
    "#Load images\n",
    "images = {\"original\": Image.open(orig_path).convert(\"RGB\")}\n",
    "for aug_name, path in aug_versions.items():\n",
    "    if os.path.exists(path):\n",
    "        images[aug_name] = Image.open(path).convert(\"RGB\")\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "columns = len(images)\n",
    "for i, (title, img) in enumerate(images.items(), start=1):\n",
    "    plt.subplot(1, columns, i)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Label: {label} â€” Base image: {base_name}\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc00cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "#count how many images per label \n",
    "\n",
    "counts_labels_new = newdf['label'].value_counts().sort_index().reset_index()\n",
    "counts_labels.columns = ['Label', 'Count']\n",
    "\n",
    "\n",
    "for i in counts_labels.iterrows():\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viualise counts in a plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15, 4))\n",
    "plt.bar(counts_labels_new['label'], counts_labels_new['count'], color = 'lightblue')\n",
    "plt.title(\"Number of Images per Bird Class\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27ec3d",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fc1e0",
   "metadata": {},
   "source": [
    "### Downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbdcbb",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1241dc",
   "metadata": {},
   "source": [
    "### Complex model (enseble methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cf80c",
   "metadata": {},
   "source": [
    "# Comparing models\n",
    "Here we need to decide what metric to use in order to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
