{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import time\n","import copy\n","from tqdm import tqdm\n","from pathlib import Path\n","import pickle\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torch.amp import autocast, GradScaler\n","\n","import torchvision\n","from torchvision import datasets, models, transforms\n","\n","from torch.utils.data import Dataset, DataLoader, random_split"],"metadata":{"id":"Jpb3XBcdyvwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MjU1TVzrkzfb"},"outputs":[],"source":["# #load files\n","if not os.path.exists(\"/content/drive\"):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","if not os.path.exists(\"/content/AppliedMLProject\"):\n","  !git clone https://YasinKaryagdi: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@github.com/YasinKaryagdi/AppliedMLProject.git\n","if not os.path.exists(\"/content/augmented_set.zip\"):\n","  !cp -r /content/drive/MyDrive/Machinelearning_files/augmented_set.zip /content/\n","  !unzip augmented_set.zip\n","if not os.path.exists(\"/content/validate_split.csv\"):\n","  !cp -r /content/drive/MyDrive/Machinelearning_files/validate_split.csv /content/\n","  !cp -r /content/drive/MyDrive/Machinelearning_files/train_augmented.csv /content/\n","  !cp -r /content/drive/MyDrive/Machinelearning_files/train_split.csv /content/\n","  !cp -r /content/drive/MyDrive/Machinelearning_files/train_balanced.csv /content/"]},{"cell_type":"code","source":["cwd = Path.cwd()\n","gitpath = cwd / \"AppliedMLProject\"\n","dirpath = gitpath / \"aml-2025-feathers-in-focus\"\n","train_images_csv = dirpath / \"train_images.csv\"\n","train_images_folder = dirpath / \"train_images\"\n","image_classes = dirpath / \"class_names.npy\"\n","drive_path = cwd / \"drive\" / \"MyDrive\" / \"Machinelearning files\"\n","val_images_csv = cwd / \"validate_split.csv\"\n","train_balanced_csv = cwd / \"train_balanced.csv\"\n"],"metadata":{"id":"NaI9baZX_ab5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Defining model and training variables\n","#use augmented trainingset and if so, use balanced set?\n","use_augmented = True\n","if use_augmented:\n","  use_balanced = True\n","  augmentations = []\n","#model\n","model_name = \"MODERNRESDEEP\" # <- modelname goes here\n","#possible models: \"M3MAX\", \"SIMPLE1\", \"CLASSIC1\", \"CLASSICRES\", \"MODERNRES\", \"MODERNRESDEEP\"\n","#use model transformations or standard\n","use_model_transforms = False\n","#use_scaler\n","use_scaler = True\n","#earlystop\n","early_stopping = True\n","patience = 5\n","min_delta = 0\n","#training batchsize\n","train_batch_size = 16\n","#validation & testing batchsize\n","val_batch_size = 32\n","#Epochs\n","num_epochs = 15\n","#Optimizer build:\n","#learningrate\n","learning_rate = 0.001\n","#momentum\n","moment = 0.9\n","#weight decay\n","wd = 0.001\n","#resize to:\n","size = (256,256)\n","#use pretrained or not\n","use_pretrained = True\n","classes = np.load(image_classes, allow_pickle=True).item()\n","num_classes = len(classes)\n","#model save name\n","model_save_name = (model_name + \"_\" +\n","                   (\"_aug\" if use_augmented else \"noaug\")+\n","                   (\"_bal\" if use_balanced else \"\")\n","                   )\n","model_save_name\n","#use seed?\n","use_seed = True\n","seed = 42\n","SEEDS = [42]"],"metadata":{"id":"3iFQhoqNq7DK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test model classes go here"],"metadata":{"id":"OwrangrexR6g"}},{"cell_type":"code","source":["class ModelM3MAX(nn.Module):\n","    def __init__(self):\n","        super(ModelM3MAX, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, stride = 1, bias=False)\n","        self.conv1_bn = nn.BatchNorm2d(32)\n","\n","        self.conv2 = nn.Conv2d(32, 48, 3, padding=1, bias=False)\n","        self.conv2_bn = nn.BatchNorm2d(48)\n","\n","        self.conv3 = nn.Conv2d(48, 64, 3, padding=1, bias=False)\n","        self.conv3_bn = nn.BatchNorm2d(64)\n","\n","        self.conv4 = nn.Conv2d(64, 80, 3, padding=1, bias=False)\n","        self.conv4_bn = nn.BatchNorm2d(80)\n","\n","        self.conv5 = nn.Conv2d(80, 96, 3, padding=1, bias=False)\n","        self.conv5_bn = nn.BatchNorm2d(96)\n","\n","        self.conv6 = nn.Conv2d(96, 112, 3, padding=1, bias=False)\n","        self.conv6_bn = nn.BatchNorm2d(112)\n","\n","        self.conv7 = nn.Conv2d(112, 128, 3, padding=1, bias=False)\n","        self.conv7_bn = nn.BatchNorm2d(128)\n","\n","        self.conv8 = nn.Conv2d(128, 144, 3, padding=1, bias=False)\n","        self.conv8_bn = nn.BatchNorm2d(144)\n","\n","        self.conv9 = nn.Conv2d(144, 160, 3, padding=1, bias=False)\n","        self.conv9_bn = nn.BatchNorm2d(160)\n","\n","        self.conv10 = nn.Conv2d(160, 176, 3, padding=1, bias=False)\n","        self.conv10_bn = nn.BatchNorm2d(176)\n","\n","        # unchanged: 176 * 8 * 8 = 11264\n","        self.fc1 = nn.Linear(11264, 200, bias=False)\n","        self.fc1_bn = nn.BatchNorm1d(200)\n","\n","    def get_logits(self, x):\n","        x = (x - 0.5) * 2.0\n","\n","        conv1 = F.relu(self.conv1_bn(self.conv1(x)))\n","        conv2 = F.relu(self.conv2_bn(self.conv2(conv1)))\n","        conv2 = F.max_pool2d(conv2, 2)  # 256 -> 128\n","\n","        conv3 = F.relu(self.conv3_bn(self.conv3(conv2)))\n","        conv4 = F.relu(self.conv4_bn(self.conv4(conv3)))\n","        conv4 = F.max_pool2d(conv4, 2)  # 128 -> 64\n","\n","        conv5 = F.relu(self.conv5_bn(self.conv5(conv4)))\n","        conv6 = F.relu(self.conv6_bn(self.conv6(conv5)))\n","        conv6 = F.max_pool2d(conv6, 2)  # 64 -> 32\n","\n","        conv7 = F.relu(self.conv7_bn(self.conv7(conv6)))\n","        conv8 = F.relu(self.conv8_bn(self.conv8(conv7)))\n","        conv8 = F.max_pool2d(conv8, 2)  # 32 -> 16\n","\n","        conv9 = F.relu(self.conv9_bn(self.conv9(conv8)))\n","        conv10 = F.relu(self.conv10_bn(self.conv10(conv9)))\n","        conv10 = F.max_pool2d(conv10, 2)  # 16 -> 8\n","\n","        # Now conv10 is (batch, 176, 8, 8)\n","        flat = torch.flatten(conv10.permute(0, 2, 3, 1), 1)\n","        logits = self.fc1_bn(self.fc1(flat))\n","        return logits\n","\n","    def forward(self, x):\n","        logits = self.get_logits(x)\n","        return logits"],"metadata":{"id":"NGj2LBX6IvdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SIMPLE1(nn.Module):\n","    def __init__(self):\n","        super(SIMPLE1, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, stride = 1, bias=False)\n","        self.conv1_bn = nn.BatchNorm2d(32)\n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1, bias=False)\n","        self.conv2_bn = nn.BatchNorm2d(64)\n","\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n","        self.conv3_bn = nn.BatchNorm2d(128)\n","\n","        self.conv4 = nn.Conv2d(128, 164, 3, padding=1, bias=False)\n","        self.conv4_bn = nn.BatchNorm2d(164)\n","\n","        self.conv5 = nn.Conv2d(164, 176, 3, padding=1, bias=False)\n","        self.conv5_bn = nn.BatchNorm2d(176)\n","\n","        # unchanged: 176 * 8 * 8 = 11264\n","        self.fc1 = nn.Linear(11264, 200, bias=False)\n","        self.fc1_bn = nn.BatchNorm1d(200)\n","\n","    def get_logits(self, x):\n","        x = (x - 0.5) * 2.0\n","\n","        #conv 1\n","        x = F.relu(self.conv1_bn(self.conv1(x)))\n","        x = F.max_pool2d(x, 2) #256 -> 128\n","\n","        #conv 2\n","        x = F.relu(self.conv2_bn(self.conv2(x)))\n","        x = F.max_pool2d(x, 2) #128 -> 64\n","\n","        #conv 3\n","        x = F.relu(self.conv3_bn(self.conv3(x)))\n","        x = F.max_pool2d(x, 2) #64 -> 32\n","\n","        # Conv 4\n","        x = F.relu(self.conv4_bn(self.conv4(x)))\n","        x = F.max_pool2d(x, 2)   # 32 -> 16\n","\n","        # Conv 5\n","        x = F.relu(self.conv5_bn(self.conv5(x)))\n","        x = F.max_pool2d(x, 2)   # 16 -> 8\n","\n","        # x is now (batch, 176, 8, 8)\n","        x = torch.flatten(x, 1)  # (batch, 11264)\n","\n","        logits = self.fc1_bn(self.fc1(x))\n","        return logits\n","\n","    def forward(self, x):\n","        logits = self.get_logits(x)\n","        return logits"],"metadata":{"id":"5gYnd_OVQ4oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CLASSIC1(nn.Module):\n","    def __init__(self):\n","        super(CLASSIC1, self).__init__()\n","\n","        # 5 stages with double conv + pooling\n","        self.stage1 = self.conv_block(3, 32)\n","        self.stage2 = self.conv_block(32, 64)\n","        self.stage3 = self.conv_block(64, 128)\n","        self.stage4 = self.conv_block(128, 256) #adapt to 256 and 512 to conform to memory norms (powers of 2)\n","        self.stage5 = self.conv_block(256, 512)\n","\n","        self.gap = nn.AdaptiveAvgPool2d(1)   # Global Average Pooling reduces parameters and betters generalization\n","        self.dropout = nn.Dropout(p=0.4) # Prevents overfitting, with some reduced probability to allow quicker learning and not over-regularize\n","        self.fc1 = nn.Linear(512, 200)\n","\n","    @staticmethod\n","    def conv_block(in_ch, out_ch): #first building block for conv layers (stack of 2)\n","        # Standard pattern: Conv -> BN -> ReLU -> Conv -> BN -> ReLU -> MaxPool\n","        return nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","\n","            nn.MaxPool2d(2)\n","        )\n","\n","    def forward(self, x): #replaced forward + logit with just forward\n","        x = (x - 0.5) * 2.0\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.stage5(x)\n","        # x is now (Batch_Size, 512, 1, 1) assuming input was 32x32 (5 max pools)\n","\n","        x = self.gap(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        logits = self.fc1(x)\n","        return logits"],"metadata":{"id":"Ffru77W9PMUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_ch)\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_ch)\n","\n","        # shortcut: identity if channels match, otherwise 1x1 conv\n","        self.shortcut = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.pool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        identity = self.shortcut(x)\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += identity\n","        out = self.relu(out)\n","        out = self.pool(out)\n","        return out\n","\n","class CLASSICRES(nn.Module):\n","    def __init__(self, num_classes=200):\n","        super().__init__()\n","\n","        # Stage-level residual blocks\n","        self.stage1 = ResidualBlock(3, 32)       # 256 -> 128\n","        self.stage2 = ResidualBlock(32, 64)      # 128 -> 64\n","        self.stage3 = ResidualBlock(64, 128)     # 64 -> 32\n","        self.stage4 = ResidualBlock(128, 256)    # 32 -> 16\n","        self.stage5 = ResidualBlock(256, 512)    # 16 -> 8\n","\n","        # Classifier\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.dropout = nn.Dropout(p=0.4)\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.stage5(x)\n","\n","        x = self.gap(x)                  # (B, 512, 1, 1)\n","        x = torch.flatten(x, 1)          # (B, 512)\n","        x = self.dropout(x)\n","        logits = self.fc(x)\n","        return logits"],"metadata":{"id":"OvLIruF1le9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SEBlock(nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super().__init__()\n","        self.fc1 = nn.Linear(channels, channels // reduction)\n","        self.fc2 = nn.Linear(channels // reduction, channels)\n","        self.activation = nn.ReLU()  # <- changed back from SiLU\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        b, c, h, w = x.size()\n","        # Squeeze: global average pooling\n","        y = x.mean(dim=(2, 3))           # (B, C)\n","        # Excitation: MLP\n","        y = self.fc2(self.activation(self.fc1(y)))  # (B, C)\n","        y = self.sigmoid(y).view(b, c, 1, 1)\n","        # Scale: multiply original feature map\n","        return x * y\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch, use_se=False, use_pool=False):\n","        super().__init__()\n","        self.use_se = use_se\n","        self.use_pool = use_pool\n","\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_ch)\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_ch)\n","\n","        self.shortcut = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1, bias=False)\n","        self.act = nn.SiLU(inplace=True)  # <-- changed from ReLU\n","        if use_pool:\n","            self.pool = nn.MaxPool2d(2)\n","        if use_se:\n","            self.se = SEBlock(out_ch)\n","\n","    def forward(self, x):\n","        identity = self.shortcut(x)\n","        out = self.act(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += identity\n","\n","        if self.use_se:\n","            out = self.se(out)\n","\n","        out = self.act(out)\n","        if self.use_pool:\n","            out = self.pool(out)\n","        return out\n","\n","\n","class MODERNRES(nn.Module):\n","    def __init__(self, num_classes=200):\n","        super().__init__()\n","\n","        # Stage-level residual blocks\n","        self.stage1 = ResidualBlock(3, 32, use_se=True)\n","        self.stage2 = ResidualBlock(32, 64, use_se=True)\n","        self.stage3 = ResidualBlock(64, 96, use_se=True)\n","        self.stage4 = ResidualBlock(96, 128, use_se=True)\n","        self.stage5 = ResidualBlock(128, 160, use_se=True)\n","\n","        # Classifier\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.dropout = nn.Dropout(p=0.4)\n","        self.fc = nn.Linear(160, num_classes)\n","\n","    def forward(self, x):\n","        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.stage5(x)\n","\n","        x = self.gap(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        logits = self.fc(x)\n","        return logits"],"metadata":{"id":"L3m6_zuw7A97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MODERNRESDEEP(nn.Module):\n","    def __init__(self, num_classes=200):\n","        super().__init__()\n","\n","        # Stage-level residual blocks\n","        self.stage1 = ResidualBlock(3, 32, use_se=True)\n","        self.stage2 = ResidualBlock(32, 64, use_se=True, use_pool = False)\n","        self.stage3 = ResidualBlock(64, 96, use_se=True)\n","        self.stage4 = ResidualBlock(96, 128, use_se=True, use_pool = False)\n","        self.stage5 = ResidualBlock(128, 160, use_se=True)\n","        self.stage6 = ResidualBlock(160, 192, use_se=True, use_pool = False)\n","        self.stage7 = ResidualBlock(192, 224, use_se=True)\n","        self.stage8 = ResidualBlock(224, 256, use_se=True, use_pool = False)\n","        self.stage9 = ResidualBlock(256, 288, use_se=True)\n","\n","        # Classifier\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.dropout = nn.Dropout(p=0.4)\n","        self.fc = nn.Linear(288, num_classes)\n","\n","    def forward(self, x):\n","        x = (x - 0.5) * 2.0  # normalize to [-1, 1]\n","\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.stage5(x)\n","        x = self.stage6(x)\n","        x = self.stage7(x)\n","        x = self.stage8(x)\n","        x = self.stage9(x)\n","\n","        x = self.gap(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        logits = self.fc(x)\n","        return logits"],"metadata":{"id":"loRPufZ4qx6O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test model activations go here"],"metadata":{"id":"m0PlFtpqr4rc"}},{"cell_type":"code","source":["# define test model+transforms here\n","if model_name == \"M3MAX\":\n","  custom_model = ModelM3MAX()\n","#gets 12.99 percent with: balanced augdata, lr 0.001, moment 0.9, wd 0.001, batchsize 32, epochs 15,\n","if model_name == \"SIMP1\":\n","  custom_model = SIMPLE1()\n","#gets 12.7 percent with: balanced augdata, lr 0.001, moment 0.9, wd 0.001, batchsize 32, epochs 15 (but fewer params, not much though)\n","if model_name == \"CLASSIC1\":\n","  custom_model = CLASSIC1()\n","#gets 15.6 percent with: balanced augdata, lr 0.001, moment 0.9, wd 0.001, batchsize 32, epochs 15 (but fewer params, not much though)\n","#but has quite some more potential with more epochs/higher learning rate/higher batch size\n","if model_name == \"CLASSICRES\":\n","  custom_model = CLASSICRES()\n","#gets 23 percent with: balanced augdata, lr 0.001, moment 0.9, wd 0.001, batchsize 64, epochs 15 (but fewer params, not much though)\n","if model_name == \"MODERNRES\":\n","  custom_model = MODERNRES()\n","#gets 23-25 percent on 30 epochs, but is much lighter than classic, trains about 2x as fast\n","if model_name == \"MODERNRESDEEP\":\n","  custom_model = MODERNRESDEEP()\n"],"metadata":{"id":"oVLnoxRIq27W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Class and function definitions goes here"],"metadata":{"id":"D_st-PCKrxwN"}},{"cell_type":"code","source":["#remove randomness for benchmarking\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","if use_seed:\n","  set_seed(seed)"],"metadata":{"id":"xIVdMKzwuzMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining dataclass\n","class CSVDataset(Dataset):\n","    def __init__(self,\n","                 csv_file,\n","                 base_dir,\n","                 transform=None,\n","                 return_id=False,\n","                 augmentation_tags=None): # Added augmentation_tags parameter\n","        self.df = pd.read_csv(csv_file)\n","\n","        # Apply augmentation filtering if tags are provided\n","        if augmentation_tags is not None:\n","            # Ensure 'original' is always included\n","            all_tags_to_include = list(set(augmentation_tags + ['original']))\n","\n","            mask = pd.Series([False] * len(self.df), index=self.df.index)\n","            for tag in all_tags_to_include:\n","                # Check if the image_path contains the augmentation tag\n","                mask = mask | self.df['image_path'].str.contains(f'_{tag}.jpg', regex=False)\n","            self.df = self.df[mask].copy()\n","\n","        self.base_dir = base_dir\n","        self.transform = transform\n","        self.return_id = return_id\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # extract fields\n","        img_id = row['id'] if self.return_id else None\n","        relative_path = row['image_path'].lstrip('/')  # safe\n","        label = row['label'] - 1   # shift to 0-based indexing\n","\n","        # build full path\n","        img_path = os.path.join(self.base_dir, relative_path)\n","\n","        # load\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # transform\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # optionally return id\n","        if self.return_id:\n","            return image, label, img_id\n","\n","        return image, label"],"metadata":{"id":"L3-GYuBXHV6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model,\n","                train_loader,\n","                val_loader,\n","                criterion,\n","                optimizer,\n","                schedular=None,\n","                num_epochs=10,\n","                early_stopping=False,\n","                epochs_no_improve=0,\n","                patience=5,\n","                min_delta=0.0,\n","                device=\"cuda\"):\n","\n","    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n","    since = time.time()\n","    model.to(device)\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    if use_scaler:\n","      scaler = GradScaler()\n","\n","    # Initialize history dictionary\n","    hist = {\n","        \"train_loss\": [],\n","        \"val_loss\": [],\n","        \"train_acc\": [],\n","        \"val_acc\": []\n","    }\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in tqdm(dataloaders_dict[phase]):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                if use_scaler:\n","                  with torch.set_grad_enabled(phase == 'train'):\n","                    with autocast(\"cuda\"):  # Mixed precision context\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","                        _, preds = torch.max(outputs, 1)\n","\n","                    if phase == 'train':\n","                        scaler.scale(loss).backward()\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                else:\n","                  with torch.set_grad_enabled(phase == 'train'):\n","                      outputs = model(inputs)\n","                      loss = criterion(outputs, labels)\n","\n","                      _, preds = torch.max(outputs, 1)\n","\n","                      if phase == 'train':\n","                          loss.backward()\n","                          optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders_dict[phase].dataset)\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","\n","            # Save metrics in history\n","            if phase == 'train':\n","                hist['train_loss'].append(epoch_loss)\n","                hist['train_acc'].append(epoch_acc.item())\n","            else:\n","                hist['val_loss'].append(epoch_loss)\n","                hist['val_acc'].append(epoch_acc.item())\n","\n","                # Early stopping logic\n","                if epoch_acc > best_acc + min_delta:\n","                    print(f\"Validation improved ({best_acc:.4f} → {epoch_acc:.4f})\")\n","                    best_acc = epoch_acc\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                    epochs_no_improve = 0\n","                else:\n","                    epochs_no_improve += 1\n","                    print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n","\n","        if epochs_no_improve >= patience and early_stopping:\n","            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n","            break\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:.4f}')\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, hist"],"metadata":{"id":"YseV73jKZVN-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most basic transformations, standardizes rgb values, resizes images to set values and converts image to tensor"],"metadata":{"id":"KF7wg35Gs5f-"}},{"cell_type":"code","source":["#Define some standard transformations\n","transformations = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((size)),\n","    transforms.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5))\n","    ])\n","## Probably better to follow the original resnet transformations\n","#See: (model.ResNet152_Weights.IMAGENET1K_V1.transforms)\n","if not use_model_transforms:\n","  model_transforms = transformations\n"],"metadata":{"id":"UzN_AvXNswB3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define datasets based on augmented or not"],"metadata":{"id":"DsCmG6nYtyrW"}},{"cell_type":"code","source":["if use_augmented == False:\n","  full_dataset = CSVDataset(\n","      csv_file=str(dirpath / \"train_images.csv\"),\n","      base_dir=str(dirpath),\n","      transform = model_transforms,\n","      return_id=False\n","  )\n","  train_size = int(split * len(full_dataset))\n","  val_size = len(full_dataset) - train_size\n","  train_dataset, val_dataset = random_split(\n","      full_dataset,\n","      [train_size, val_size],\n","      generator=torch.Generator().manual_seed(seed)\n","  )\n","  loader = DataLoader(full_dataset, batch_size=train_batch_size, shuffle=True)"],"metadata":{"id":"vLezHkjGs4Zk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if use_augmented == True:\n","  if use_balanced:\n","    train_dataset = CSVDataset(\n","        csv_file=str(cwd / \"train_balanced.csv\"),\n","        base_dir=str(cwd),\n","        transform = model_transforms,\n","        return_id=False\n","    )\n","  else:\n","    train_dataset = CSVDataset(\n","        csv_file=str(cwd / \"train_augmented.csv\"),\n","        base_dir=str(cwd),\n","        transform = model_transforms,\n","        return_id=False\n","    )\n","  val_dataset = CSVDataset(\n","      csv_file=str(val_images_csv),\n","      base_dir=str(dirpath),\n","      transform = model_transforms,\n","      return_id=False\n","  )"],"metadata":{"id":"N5gGA7XZtxyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#define dataloaders\n","# data loaders\n","#create full loader\n","train_loader = DataLoader(train_dataset,\n","                          batch_size=train_batch_size,\n","                          shuffle=True,\n","                          num_workers=2,\n","                          pin_memory=True,\n","                          prefetch_factor=2,\n","                          persistent_workers=True)\n","val_loader = DataLoader(val_dataset,\n","                        batch_size=val_batch_size,\n","                        shuffle=False,\n","                        num_workers=2,\n","                        pin_memory=True,\n","                        prefetch_factor=2,\n","                        persistent_workers=True\n","                        )"],"metadata":{"id":"aCKiE6mhDpgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Detect if we have a GPU available\n","torch.cuda.empty_cache()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFa_mDfNv8JS","executionInfo":{"status":"ok","timestamp":1765713931958,"user_tz":-60,"elapsed":6,"user":{"displayName":"Mark Schuddeboom","userId":"03719376082377648878"}},"outputId":"a44af5f8-a17f-44bf-bcc4-c79d0cd45171"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["#gather optimizable parameters\n","params_to_update = custom_model.parameters()\n","#Design optimzer\n","# optimizer = optim.SGD(params_to_update, lr=learning_rate, momentum=moment,\n","#                       weight_decay=wd\n","#                       )\n","optimizer = optim.AdamW(params_to_update, lr=learning_rate)\n","# Setup the loss func\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"uv1NVePuv8uU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for seed in SEEDS:\n","  set_seed(seed)\n","  #initialize model helpers\n","  if model_name == \"M3MAX\":\n","    custom_model = ModelM3MAX()\n","  if model_name == \"SIMP1\":\n","    custom_model = SIMPLE1()\n","  if model_name == \"CLASSIC1\":\n","    custom_model = CLASSIC1()\n","  if model_name == \"CLASSICRES\":\n","    custom_model = CLASSICRES()\n","  if model_name == \"MODERNRES\":\n","    custom_model = MODERNRES()\n","  if model_name == \"MODERNRESDEEP\":\n","    custom_model = MODERNRESDEEP()\n","\n","  #initialize optimzer and criterion\n","  # Initialize optimizer and criterion\n","  params_to_update = custom_model.parameters()\n","  optimizer = optim.AdamW(params_to_update, lr=learning_rate)\n","  criterion = nn.CrossEntropyLoss()\n","\n","  #train model\n","  model_trained, hist = train_model(custom_model,\n","                              train_loader,\n","                              val_loader,\n","                              criterion,\n","                              optimizer,\n","                              early_stopping = early_stopping,\n","                              patience = patience,\n","                              min_delta = min_delta,\n","                              schedular=None,\n","                              num_epochs=num_epochs,\n","                              device=device)\n","  torch.save(model_trained.state_dict(), f\"/content/drive/MyDrive/Test/{model_name}_{seed}.pth\")\n","  with open(f\"/content/drive/MyDrive/Test/{model_name}_{seed}_acc.pkl\", \"wb\") as f:\n","        pickle.dump(hist, f)\n","\n","  print(f\"Seed {seed}: model and history saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eB6Xskixx6ST","executionInfo":{"status":"ok","timestamp":1765721079610,"user_tz":-60,"elapsed":7147631,"user":{"displayName":"Mark Schuddeboom","userId":"03719376082377648878"}},"outputId":"1282b0a9-fa03-4704-b78b-ef90e4a09d03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 5.0758 Acc: 0.0133\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:06<00:00,  3.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.8974 Acc: 0.0178\n","Validation improved (0.0000 → 0.0178)\n","\n","Epoch 2/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 4.7664 Acc: 0.0296\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.7888 Acc: 0.0216\n","Validation improved (0.0178 → 0.0216)\n","\n","Epoch 3/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:51<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 4.5184 Acc: 0.0456\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.6096 Acc: 0.0318\n","Validation improved (0.0216 → 0.0318)\n","\n","Epoch 4/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 4.3049 Acc: 0.0658\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.5974 Acc: 0.0407\n","Validation improved (0.0318 → 0.0407)\n","\n","Epoch 5/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:51<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 4.0620 Acc: 0.0966\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.5574 Acc: 0.0522\n","Validation improved (0.0407 → 0.0522)\n","\n","Epoch 6/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 3.8337 Acc: 0.1269\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.6579 Acc: 0.0573\n","Validation improved (0.0522 → 0.0573)\n","\n","Epoch 7/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 3.5880 Acc: 0.1655\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.6105 Acc: 0.0712\n","Validation improved (0.0573 → 0.0712)\n","\n","Epoch 8/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 3.3463 Acc: 0.2075\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.6648 Acc: 0.0776\n","Validation improved (0.0712 → 0.0776)\n","\n","Epoch 9/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 3.0962 Acc: 0.2491\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.7536 Acc: 0.0827\n","Validation improved (0.0776 → 0.0827)\n","\n","Epoch 10/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.8979 Acc: 0.2901\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.8099 Acc: 0.1018\n","Validation improved (0.0827 → 0.1018)\n","\n","Epoch 11/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.6835 Acc: 0.3376\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 4.8947 Acc: 0.0865\n","No improvement for 1 epoch(s).\n","\n","Epoch 12/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.5058 Acc: 0.3752\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 5.0015 Acc: 0.0941\n","No improvement for 2 epoch(s).\n","\n","Epoch 13/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.3455 Acc: 0.4085\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 5.0197 Acc: 0.0954\n","No improvement for 3 epoch(s).\n","\n","Epoch 14/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.1920 Acc: 0.4428\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 5.1899 Acc: 0.0980\n","No improvement for 4 epoch(s).\n","\n","Epoch 15/15\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1250/1250 [07:50<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 2.0576 Acc: 0.4733\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 5.2323 Acc: 0.1005\n","No improvement for 5 epoch(s).\n","Early stopping triggered at epoch 15!\n","Training complete in 119m 7s\n","Best val Acc: 0.1018\n","Seed 42: model and history saved.\n"]}]},{"cell_type":"code","source":["# Example input\n","batchsize = 1\n","example_input = torch.randn(batchsize, 3, 256, 256)\n","\n","# Init model\n","curr_model = TEMP()\n","curr_model.eval()\n","\n","# compute flops\n","flops = FlopCountAnalysis(curr_model, example_input)\n","fw_flops = flops.total()\n","\n","# create dataframe to store results\n","data = []\n","data.append({\n","    'model_name': 'baseline',\n","    'fw_flops': fw_flops,\n","})\n","\n","# init model\n","curr_model = MODERNRESS()\n","curr_model.eval()\n","\n","# compute flops\n","flops = FlopCountAnalysis(curr_model, example_input)\n","fw_flops = flops.total()\n","\n","data.append({\n","    'model_name': 'base model',\n","    'fw_flops': fw_flops,\n","})\n","\n","\n","# init model\n","currmodel = MODERNRESS()\n","modelList = [currmodel] * 7\n","curr_model = EnsembleModel(modelList)\n","curr_model.eval()\n","\n","# freeze parameters of individual models\n","for param in curr_model.parameters():\n","    param.requires_grad = False\n","\n","# unfreeze parameters of the classifier\n","for param in curr_model.classifier.parameters():\n","    param.requires_grad = True\n","curr_model.eval()\n","\n","# compute flops\n","flops = FlopCountAnalysis(curr_model, example_input)\n","fw_flops = flops.total()\n","\n","data.append({\n","    'model_name': 'ensembled',\n","    'fw_flops': fw_flops,\n","})\n","\n","df = pd.DataFrame(data)\n","\n","print(df)\n","\n","# plot\n","fig, ax = plt.subplots(figsize=(8,6))\n","ax.bar(df['model_name'], df['fw_flops']/1e9)\n","ax.set_ylabel('FLOPs (billions)')\n","ax.set_title('Total forward FLOPs for models')\n","plt.show()\n"],"metadata":{"id":"AM_6eWJvMCaL"},"execution_count":null,"outputs":[]}]}